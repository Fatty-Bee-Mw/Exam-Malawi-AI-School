# Malawian Educational AI Tutor (Local, CPU-Friendly)

This project provides a **local Retrieval-Augmented Generation (RAG) tutor** that
answers questions **strictly using your Malawian textbooks**. It runs fully on
CPU (no GPU required) and exposes a simple FastAPI endpoint `/ask`.

## Folder structure

- `malawian_educational_ai_tutor/`
  - `config.py` – paths and model settings (textbook folder, chunk size, etc.).
  - `ingest.py` – scanning and text loading + chunking.
  - `build_index.py` – builds the FAISS vector index from textbooks.
  - `rag_llm.py` – RAG core with TinyLLaMA (CPU) for question answering.
  - `api.py` – FastAPI app exposing `/ask`.
  - `test_client.py` – simple script to test `/ask` locally.
  - `requirements.txt` – Python dependencies for this tutor.
  - `data/`
    - `textbooks/` – put your TXT/PDF/DOC/DOCX textbooks here.
    - `index/` – generated FAISS index, embeddings, and metadata.

---

## 1. Installation (CPU-only)

From the project root (`h:/Exam-AI-Mw Schools`):

```bash
python -m venv venv
venv\Scripts\activate  # on Windows
pip install --upgrade pip
pip install -r malawian_educational_ai_tutor/requirements.txt
```

> **Note (FAISS on Windows):**
> The `faiss-cpu` package may require Conda or WSL on Windows. If `pip install faiss-cpu` fails, use:
>
> ```bash
> conda install -c pytorch faiss-cpu
> ```
>
> or run this project from WSL/Ubuntu where `faiss-cpu` wheels are available.

The first time you run the LLM it will download **TinyLLaMA (≈1.1B params)**
from HuggingFace; this may take a few minutes and requires an internet
connection once.

---

## 2. Add textbooks

1. Ensure the directories exist (they are created automatically when needed):

   - `malawian_educational_ai_tutor/data/textbooks/`
   - `malawian_educational_ai_tutor/data/index/`

2. Copy your textbooks into `data/textbooks/`:

   - Supported by design: `*.txt`, `*.pdf`, `*.docx`, `*.doc`
   - Other extensions are **skipped** and recorded as unsupported.

3. For `.doc` support, make sure `textract` and its system dependencies are
   installed. Otherwise `.doc` files will be skipped and noted as unsupported.

---

## 3. Build (or update) the FAISS index

Run the index builder:

```bash
venv\Scripts\activate  # if not already active
python -m malawian_educational_ai_tutor.build_index
```

This will:

- Scan `data/textbooks/` for TXT/PDF/DOCX/DOC files.
- Extract text from each file.
- Split each textbook into ~500-word chunks (with overlap).
- Compute **sentence-transformers/all-MiniLM-L6-v2** embeddings on CPU.
- Cache embeddings **per file**, reusing them on the next run when the file
  has not changed.
- Build a **FAISS IndexFlatIP** vector index and store it under
  `data/index/` (FAISS index + docstore + metadata).

The script outputs JSON metadata including any **unsupported** files (due to
missing dependencies, unreadable PDFs, unsupported extensions, etc.).

To **add new textbooks or update existing ones** later:

1. Place or update files in `data/textbooks/`.
2. Re-run:

   ```bash
   python -m malawian_educational_ai_tutor.build_index
   ```

Only changed/new files will have embeddings recomputed; unchanged textbooks
reuse their cached embeddings for faster updates.

---

## 4. Run the local API (`/ask`)

Start the FastAPI app with Uvicorn:

```bash
venv\Scripts\activate
uvicorn malawian_educational_ai_tutor.api:app --host 0.0.0.0 --port 8000
```

The server will lazily load:

- The FAISS index and docstore from `data/index/`.
- The embedding model `all-MiniLM-L6-v2`.
- The TinyLLaMA model `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (CPU).

### `/ask` endpoint

- **Method:** `POST`
- **Path:** `/ask`
- **Request JSON:**

  ```json
  { "question": "What is the difference between weather and climate?" }
  ```

- **Response JSON:**

  ```json
  { "answer": "..." }
  ```

The answer is generated by:

1. Embedding your question.
2. Retrieving the most relevant textbook chunks from FAISS.
3. Building a prompt that contains **only those chunks as CONTEXT**.
4. Asking TinyLLaMA to answer **strictly from that context**. If the context
   does not contain the answer, the tutor says so.

---

## 5. Test script

With the API running on `http://127.0.0.1:8000`:

```bash
venv\Scripts\activate
python -m malawian_educational_ai_tutor.test_client
```

This sends a sample question to `/ask` and prints the returned answer.

---

## 6. How the RAG pipeline works

1. **Ingestion** (`ingest.py`, `build_index.py`)
   - Discover textbooks in `data/textbooks/`.
   - Load text via TXT/PDF/DOCX/DOC readers.
   - Split into ~500-word overlapping chunks.
   - Compute or reuse per-file embeddings using
     `sentence-transformers/all-MiniLM-L6-v2` (CPU-only).
   - Build a FAISS vector index (IndexFlatIP on normalized embeddings).

2. **Retrieval + Generation** (`rag_llm.py`)
   - When a question arrives, embed it with the same model.
   - Use FAISS to get the top-k similar chunks.
   - Concatenate chunks into a context window (limited by `MAX_CONTEXT_CHARS`).
   - Construct an instruction-style prompt for TinyLLaMA that:
     - Emphasizes using **only textbook context**.
     - Tells the model to admit when the answer is not present.
   - Generate an answer with conservative decoding parameters
     (`MAX_NEW_TOKENS`, `TEMPERATURE`, `TOP_P`).

3. **API layer** (`api.py`)
   - Exposes `/ask` for JSON questions and answers.

---

## 7. Adding new textbooks in the future

When you receive new Malawian textbooks or updated versions:

1. **Drop the files** into `malawian_educational_ai_tutor/data/textbooks/`.
2. **Rebuild the index**:

   ```bash
   python -m malawian_educational_ai_tutor.build_index
   ```

3. The system will:
   - Detect which files are new or changed.
   - Recompute embeddings **only** for those files.
   - Reuse cached embeddings for unchanged textbooks.
   - Rebuild the FAISS index using the cached + new embeddings.

4. Restart the FastAPI server if it is running so it reloads the updated index.

---

## 8. Optional: Frontend / Online access

### 8.1. Simple Netlify frontend

You can build a minimal web UI (e.g. React) that calls the `/ask` endpoint.
A typical flow:

1. **Frontend** (React/Vite or your existing `src/` React app):
   - Create a simple form with a textarea for `question` and a button.
   - `fetch("https://YOUR_PUBLIC_URL/ask", { method: "POST", body: JSON.stringify({ question }) })`.
   - Display the `answer` returned by the API.

2. **Deploy frontend to Netlify**
   - Use the existing `netlify.toml` or create a new site pointing to your
     frontend build output.
   - Configure the frontend to call your **public API URL** (see Cloudflare
     Tunnel or reverse proxy below).

### 8.2. Cloudflare Tunnel for exposing the local API

To access the tutor from the internet without opening ports directly:

1. Install `cloudflared` (Cloudflare Tunnel CLI) and log in.
2. Start your FastAPI server locally on port `8000`.
3. Run:

   ```bash
   cloudflared tunnel --url http://localhost:8000
   ```

4. Cloudflare will provide a public HTTPS URL like
   `https://something.trycloudflare.com`.
5. Configure your frontend (Netlify or otherwise) to call
   `https://something.trycloudflare.com/ask`.

This keeps your tutor running locally while making it reachable from the web.

---

## 9. Notes and customization

- Adjust settings in `config.py`:
  - `CHUNK_SIZE_WORDS`, `CHUNK_OVERLAP_WORDS` for chunking.
  - `TOP_K_CHUNKS` for how many chunks to retrieve.
  - `MAX_CONTEXT_CHARS` to control prompt length.
  - Generation parameters: `MAX_NEW_TOKENS`, `TEMPERATURE`, `TOP_P`.
- You can swap TinyLLaMA for any other CPU-friendly HuggingFace model by
  changing `LLM_MODEL_NAME` in `config.py` (and rebuilding the environment if
  needed).

You now have a **CPU-friendly, local Malawian Educational AI Tutor** that uses
**your textbooks as ground truth**, with a clear path to extend it or deploy a
frontend on top.
